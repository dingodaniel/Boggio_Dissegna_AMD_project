{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "progettoimdb.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt4rWHHlt5C7"
      },
      "source": [
        "Setting Kaggle by creating directory /root/.kaggle/\n",
        "\n",
        "Creating file kaggle.json with username and key\n",
        "\n",
        "Setting the rights of the file in order to be readable and writeable only by this user\n",
        "\n",
        "(We know that this is not a safe way to do it because it shows username and key in clear, but those are accademic-purpose informations so we felt like show them)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtnZ7o_uZ8IA"
      },
      "source": [
        "!mkdir /root/.kaggle/\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as writefile:\n",
        "    writefile.write('{\\\"username\\\":\\\"albertoboggio\\\",\\\"key\\\":\\\"51842ae4d703c2aa0fd682ba4a620a79\\\"}')\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hl88Vbkeun1U"
      },
      "source": [
        "Downloading the dataset IMDB and decompressing the file title.principals.tsv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCPy8umxeMd4",
        "outputId": "ea591a73-9e01-4f15-dbc0-9dc24c613ed3"
      },
      "source": [
        "!kaggle datasets download ashirwadsangwan/imdb-dataset --force\n",
        "!unzip imdb-dataset.zip title.principals.tsv.gz\n",
        "!gunzip -k title.principals.tsv.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading imdb-dataset.zip to /content\n",
            "100% 1.44G/1.44G [00:09<00:00, 137MB/s]\n",
            "100% 1.44G/1.44G [00:09<00:00, 155MB/s]\n",
            "Archive:  imdb-dataset.zip\n",
            "replace title.principals.tsv.gz? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: title.principals.tsv.gz  \n",
            "gzip: title.principals.tsv already exists; do you wish to overwrite (y or n)? ^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I51nl7ZbuxWi"
      },
      "source": [
        "This function reads the file containing the baskets and extract from it a list of baskets (represented by lists of items) where only actors appear as items."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2mD_AaWhLVS"
      },
      "source": [
        "def load_baskets(f_name):\n",
        "    \"\"\"\n",
        "        Take the f_name (filename) of the file in .tsv format\n",
        "        and loads the baskets in main memory.\n",
        "        Return a list of lists.\n",
        "        Every list is a basket and contains the items sorted by id.\n",
        "    \"\"\"\n",
        "\n",
        "    with open(f_name, mode=\"r\", encoding='utf8') as f:\n",
        "\n",
        "        baskets = list()\n",
        "        curr_basket = '1'\n",
        "        b = list() # Contains the items of the curr_basket\n",
        "\n",
        "        f.readline()\n",
        "        \n",
        "        for row in f:\n",
        "            row_data = row.strip().split('\\t')\n",
        "            basket = (row_data[0][2:]).lstrip('0')\n",
        "            item = (row_data[2][2:]).lstrip('0')\n",
        "\n",
        "            \"\"\"\n",
        "            We can do the following because we know that the datasets is sorted\n",
        "            according to the movie index, so we know that whenever the movie \n",
        "            changes it will be a new movie not already processed\n",
        "            \"\"\"\n",
        "            if(row_data[3] == \"actor\" or row_data[3] == \"actress\"):\n",
        "                if(curr_basket != basket):\n",
        "                    if(len(b) != 0):\n",
        "                        baskets.append(sorted(b))\n",
        "                    b = list()\n",
        "                    curr_basket = basket\n",
        "\n",
        "                b.append(int(item))\n",
        "\n",
        "        baskets.append(sorted(b))\n",
        "\n",
        "    return baskets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmXcxrsm_-_9"
      },
      "source": [
        "In the following code we calculate the number of:\n",
        "1. items (I)\n",
        "2. baskets (B)\n",
        "3. possible pairs (P)\n",
        "4. worst case of different pairs in baskets (WP)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjY1czEjALAw",
        "outputId": "0e4c35b1-1877-494e-8c71-3c225d2692bf"
      },
      "source": [
        "import scipy.special\n",
        "\n",
        "baskets = load_baskets('title.principals.tsv')\n",
        "\n",
        "I = len(set([item for b in baskets for item in b]))#number of items\n",
        "\n",
        "B = len(baskets)#number of baskets\n",
        "\n",
        "P = scipy.special.comb(I, 2)#number of possible pairs\n",
        "\n",
        "WP = sum([scipy.special.comb(len(b), 2) for b in baskets])#worst case on the n. of different pairs in baskets\n",
        "\n",
        "print(f\"I={I}\", f\"B={B}\", f\"P={P}\", f\"WP={WP}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I=1868056 B=3697162 P=1744815675540.0 WP=31901860.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxwYp-iDrase"
      },
      "source": [
        "The following functions are useful for the implementation of the Apriori algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8LrmAZBrJSv"
      },
      "source": [
        "import itertools as it\n",
        "import scipy.special\n",
        "\n",
        "def pair_to_index(n, inverted=False):\n",
        "    \"\"\"\n",
        "        n is the maximum number of indexes\n",
        "    \"\"\"\n",
        "    def p_t_i(pair):\n",
        "        if inverted:\n",
        "            j, i = pair\n",
        "        else:\n",
        "            i, j = pair\n",
        "            \n",
        "        return int((n*(n-1)/2) - (n-i)*((n-i)-1)/2 + j - i - 1)\n",
        "    \n",
        "    return p_t_i\n",
        "    \n",
        "def index_to_pair(n):\n",
        "    \"\"\"\n",
        "        n is the maximum number of indexes\n",
        "    \"\"\"\n",
        "    def i_t_p(k):\n",
        "        i = n - 2 - int((-8*k + 4*n*(n-1)-7)**(1/2)/2.0 - 0.5)\n",
        "        j = k + i + 1 - n*(n-1)/2 + (n-i)*((n-i)-1)/2\n",
        "        return (int(i), int(j))\n",
        "    \n",
        "    return i_t_p\n",
        "\n",
        "def filt(cand_sets, supp_thr):\n",
        "    \"\"\"\n",
        "        Return a set with the frequent subsets\n",
        "        in cand_sets by checking whether their count is >=\n",
        "        than the threshold.\n",
        "    \"\"\"\n",
        "    return set([kset for kset, count in cand_sets.items() if count >= supp_thr])\n",
        "\n",
        "def filt_pairs(cand_pairs, supp_thr, num_of_items):\n",
        "    \"\"\"\n",
        "        Return the set of frequent pairs\n",
        "        by checking whether their count is >=\n",
        "        than the support threshold.\n",
        "    \"\"\"\n",
        "\n",
        "    #Initialize the function for the conversion k->(i,j)\n",
        "    i_t_p = index_to_pair(num_of_items)\n",
        "    return set([i_t_p(index) for index, count in cand_pairs.items() if count >= supp_thr])\n",
        "\n",
        "def pair_constr(freq_singles, baskets, num_of_items):\n",
        "    \"\"\"\n",
        "        Takes the set of frequent singlentons, baskets\n",
        "        and the max index of the items and returns a dictionary \n",
        "        with keys the candidate pairs and values their counts.\n",
        "    \"\"\"\n",
        "\n",
        "    #Initialize the functions for the conversion (i,j)<->k\n",
        "    p_t_i = pair_to_index(num_of_items)\n",
        "    i_t_p = index_to_pair(num_of_items)\n",
        "                          \n",
        "    cand_pairs = dict()\n",
        "\n",
        "    #Remove the not frequent singles from the baskets\n",
        "    baskets[:] = [[item for item in b if (item,) in freq_singles] for b in baskets if len(b) > 0]\n",
        "\n",
        "    #Construct the candidate pairs\n",
        "    for b in baskets:\n",
        "        for pair in it.combinations(b, 2):\n",
        "            index = p_t_i(pair)\n",
        "            if index not in cand_pairs:\n",
        "                cand_pairs[index] = 1\n",
        "            else:\n",
        "                cand_pairs[index] += 1\n",
        "\n",
        "    return cand_pairs\n",
        "\n",
        "def tuple_in_basket(ktuple, basket):\n",
        "    \"\"\"\n",
        "        Returns whether all the items in the\n",
        "        k-tuple are in the basket or not.\n",
        "    \"\"\"\n",
        "    for item in ktuple:\n",
        "        if item not in basket:\n",
        "            return False\n",
        "    \n",
        "    return True\n",
        "\n",
        "def itemfilter1(baskets, freq_sets, k):\n",
        "    \"\"\"\n",
        "        Takes the baskets, the frequent sets of the previous\n",
        "        iteration and k (index of the current iteration) and \n",
        "        returns a new list of baskets by filtering out the not\n",
        "        necessary items from the baskets.\n",
        "        This is a relaxation of the standard filtering (itemfilter2).\n",
        "    \"\"\"\n",
        "\n",
        "    # Find  the items which appear in at least k-1 (k-1)-tuples\n",
        "    count_items = dict()\n",
        "    good_items = set()\n",
        "\n",
        "    for k1tuple in freq_sets:\n",
        "        for item in k1tuple:\n",
        "            if item not in count_items:\n",
        "                count_items[item] = 1\n",
        "            else:\n",
        "                count_items[item] += 1\n",
        "\n",
        "    for item in count_items:\n",
        "        if count_items[item] >= k-1:\n",
        "            good_items.add(item)\n",
        "\n",
        "    del count_items\n",
        "    gc.collect()\n",
        "\n",
        "    #Update the baskets based on the freq_sets\n",
        "    new_baskets = list()\n",
        "\n",
        "    for b in baskets:\n",
        "        new_b = list()\n",
        "\n",
        "        #Use only the necessary items\n",
        "        for item in b:\n",
        "\n",
        "            if item in good_items:\n",
        "                new_b.append(item)\n",
        "\n",
        "        if len(new_b) > 0:\n",
        "            new_baskets.append(new_b)\n",
        "    \n",
        "    return new_baskets\n",
        "\n",
        "def itemfilter2(baskets, freq_sets, k):\n",
        "    \"\"\"\n",
        "        Takes the baskets, the frequent sets of the previous\n",
        "        iteration and k (index of the current iteration) and \n",
        "        returns a new list of baskets by filtering out the not\n",
        "        necessary items from the baskets.\n",
        "    \"\"\"\n",
        "\n",
        "    new_baskets = list()\n",
        "\n",
        "    for b in baskets:\n",
        "        new_b = list()\n",
        "\n",
        "        #Chooses the filter method for the current basket    \n",
        "        if len(freq_sets) < scipy.special.comb(len(b), k-1):\n",
        "            #Starts from the items in b and delete them if the condition is not met\n",
        "            for item in b:\n",
        "                count = 0\n",
        "                \n",
        "                for k1tuple in freq_sets:\n",
        "                    if item in k1tuple and tuple_in_basket(k1tuple, b):\n",
        "                        count += 1\n",
        "                if count >= k-1:\n",
        "                    new_b.append(item)\n",
        "            \n",
        "        else:\n",
        "            #Starts from the k-1tuples in the basket, count their items\n",
        "            #if it's frequent and then delete the items\n",
        "            count_items = dict()\n",
        "\n",
        "            for k1tuple in it.combinations(b, k-1):\n",
        "                if k1tuple in freq_sets:\n",
        "                    for item in k1tuple:\n",
        "                        if item not in count_items:\n",
        "                            count_items[item] = 1\n",
        "                        else:\n",
        "                            count_items[item] += 1\n",
        "            \n",
        "            for item in b:\n",
        "                if item in count_items:\n",
        "                    if count_items[item] >= k-1:\n",
        "                        new_b.append(item)\n",
        "            \n",
        "        if len(new_b) > 0:\n",
        "            new_baskets.append(new_b)\n",
        "\n",
        "    return new_baskets\n",
        "\n",
        "\n",
        "def construct(freq_sets, baskets, k, itemfilter=1):\n",
        "    \"\"\"\n",
        "        Take as input the list of k-1 frequent tuples\n",
        "        and returns a dictionary with the\n",
        "        candidate k tuples and their count.\n",
        "        Works for k>=3.\n",
        "        itemfilter goes from 0 to 2:\n",
        "          0 = no filtering\n",
        "          1 = relaxation of the standard filt.\n",
        "          2 = standard filtering\n",
        "    \"\"\"\n",
        "\n",
        "    cand_sets = dict()\n",
        "\n",
        "    if itemfilter == 1:\n",
        "\n",
        "        baskets[:] = itemfilter1(baskets, freq_sets, k)\n",
        "    \n",
        "    elif itemfilter == 2:\n",
        "        \n",
        "        baskets[:] = itemfilter2(baskets, freq_sets, k)\n",
        "\n",
        "    for b in baskets:\n",
        "        #Construct the candidate sets\n",
        "        for kset in it.combinations(b, k):\n",
        "            if kset not in cand_sets:\n",
        "                cand_sets[kset] = 1\n",
        "            else:\n",
        "                cand_sets[kset] += 1\n",
        "\n",
        "    return cand_sets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CNf_U7uqwbl"
      },
      "source": [
        "This is an implementation of Apriori algorithm having as input parameters:\n",
        "1. the set of baskets (returned by the load_baskets function)\n",
        "2. the support threshold\n",
        "3. the minimal and the maximal cardinality of the frequent itemsets to be returned (optional)\n",
        "4. the verbose flag which specifies the will to print computational resources' details (optional)\n",
        "5. the aggressiveness of the item filtering (the one before the counting phase) (optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QITpjQFsqf02"
      },
      "source": [
        "import gc\n",
        "import sys\n",
        "import time\n",
        "\n",
        "def apriori(baskets, supp_thr, min_card = 2, max_card = float('inf'), verbose = False, itemfilter=0):\n",
        "    \n",
        "    #List of frequent sets\n",
        "    all_freq_sets = list()\n",
        "\n",
        "    #Init. the table of the current iteration candidate sets\n",
        "    #This will contain the couts of the candidate sets\n",
        "    cand_sets = dict()\n",
        "\n",
        "    t = time.time()\n",
        "    #FIRST PASS\n",
        "    #Count the singletons in every basket\n",
        "    for b in baskets:\n",
        "        for single in it.combinations(b, 1):\n",
        "            if single not in cand_sets:\n",
        "                cand_sets[single] = 1\n",
        "            else:\n",
        "                cand_sets[single] += 1\n",
        "\n",
        "    if(verbose):\n",
        "        print(f\"Memory used to count singletons:\", sys.getsizeof(cand_sets)/1000000,\"MB\")\n",
        "        print(\"Number of candidate singletons:\", len(cand_sets))\n",
        "\n",
        "    #Filter out the not-frequent singletons\n",
        "    freq_sets = filt(cand_sets, supp_thr)\n",
        "\n",
        "    if(verbose):\n",
        "        print(\"Number of frequent singletons:\", len(freq_sets))\n",
        "        print(\"Time to count singletons:\", time.time() - t, \"sec\\n\")\n",
        "\n",
        "    #Deallocate the counts for singletons\n",
        "    del cand_sets\n",
        "    gc.collect()\n",
        "\n",
        "    #Save the freq. singletons in the list to return if necessary\n",
        "    if(min_card == 1):\n",
        "        all_freq_sets.extend(freq_sets)\n",
        "\n",
        "    #SECOND PASS\n",
        "    #Calculate the num. of possible pairs\n",
        "    num_of_items = max([max(b) for b in baskets])\n",
        "\n",
        "    t = time.time()\n",
        "    #Count the pairs in every basket\n",
        "    cand_sets = pair_constr(freq_sets, baskets, num_of_items)\n",
        "\n",
        "    if(verbose):\n",
        "        print(f\"Memory used to count pairs:\", sys.getsizeof(cand_sets)/1000000,\"MB\")\n",
        "        print(\"Number of candidate pairs:\", len(cand_sets))\n",
        "\n",
        "    #Filter out the not-frequent pairs\n",
        "    freq_sets = filt_pairs(cand_sets, supp_thr, num_of_items)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Number of frequent pairs:\", len(freq_sets))\n",
        "        print(\"Time to count pairs:\", time.time() - t, \"sec\\n\")\n",
        "\n",
        "    #Deallocate the counts for pairs\n",
        "    del cand_sets\n",
        "    gc.collect()\n",
        "    \n",
        "    #Save the freq. pairs in the list to return if necessary\n",
        "    if(min_card <= 2 and 2 <= max_card):\n",
        "        all_freq_sets.extend(freq_sets)\n",
        "\n",
        "    #K-TH PASS with k>=3\n",
        "    k = 3\n",
        "    while len(freq_sets) != 0 and k <= max_card:\n",
        "        t = time.time()\n",
        "\n",
        "        cand_sets = construct(freq_sets, baskets, k, itemfilter=itemfilter)\n",
        "        if(verbose):\n",
        "            print(f\"Memory used to count {k}sets:\", sys.getsizeof(cand_sets)/1000000,\"MB\")\n",
        "            print(f\"Number of candidate {k}sets:\", len(cand_sets))\n",
        "\n",
        "        #Filter out the not-frequent k-sets\n",
        "        freq_sets = filt(cand_sets, supp_thr)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Number of frequent {k}sets:\", len(freq_sets))\n",
        "            print(f\"Time to count {k}sets:\", time.time() - t,\"sec\\n\")\n",
        "\n",
        "        #Deallocate the counts for k-sets\n",
        "        del cand_sets\n",
        "        gc.collect()\n",
        "\n",
        "        if(min_card <= k and len(freq_sets) != 0):\n",
        "            all_freq_sets.extend(freq_sets)\n",
        "        \n",
        "        k += 1\n",
        "\n",
        "    return all_freq_sets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll15WK_6Lnl1"
      },
      "source": [
        "This is an implementation of the PCY. The parameters are the same of Apriori algorithm but is required an additional parameter (\"buckets\") which specifies the number of buckets to use (cardinality of the image of the hash functions)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKbIinnELmDn"
      },
      "source": [
        "def pcy(baskets, supp_thr, buckets, min_card = 2, max_card = float('inf'), verbose = False, itemfilter=1):\n",
        "\n",
        "    #Calculate the num. of possible pairs\n",
        "    num_of_items = max([max(b) for b in baskets])\n",
        "    \n",
        "    #List of frequent sets\n",
        "    all_freq_sets = list()\n",
        "\n",
        "    #This will contain the couts of the candidate sets\n",
        "    cand_sets = dict()\n",
        "    #These will contain the counts of the buckets\n",
        "    cand_buck1 = dict()\n",
        "    cand_buck2 = dict()\n",
        "\n",
        "    #Functions used to calcute the 2 hash\n",
        "    p_t_i = pair_to_index(num_of_items)\n",
        "    p_t_i2 = pair_to_index(num_of_items, inverted=True)\n",
        "\n",
        "    t = time.time()\n",
        "    #FIRST PASS\n",
        "    #Count the singletons in every basket\n",
        "    for b in baskets:\n",
        "        for single in it.combinations(b, 1):\n",
        "            if single not in cand_sets:\n",
        "                cand_sets[single] = 1\n",
        "            else:\n",
        "                cand_sets[single] += 1\n",
        "        \n",
        "        # Count the candidate buckets of pairs using p_t_i as hash function\n",
        "        for pair in it.combinations(b, 2):\n",
        "            buck1 = p_t_i(pair) % buckets\n",
        "            buck2 = p_t_i2(pair) % buckets\n",
        "            if buck1 not in cand_buck1:\n",
        "                cand_buck1[buck1] = 1\n",
        "            else:\n",
        "                cand_buck1[buck1] += 1\n",
        "            \n",
        "            if buck2 not in cand_buck2:\n",
        "                cand_buck2[buck2] = 1\n",
        "            else:\n",
        "                cand_buck2[buck2] += 1\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Memory used to count singletons:\", sys.getsizeof(cand_sets)/1000000,\"MB\")\n",
        "        print(f\"Memory used to count buckets 1:\", sys.getsizeof(cand_buck1)/1000000,\"MB\")\n",
        "        print(f\"Memory used to count buckets 2:\", sys.getsizeof(cand_buck2)/1000000,\"MB\")\n",
        "        print(\"Number of candidate singletons:\", len(cand_sets))\n",
        "\n",
        "    #Initialization of the 2 bitmap\n",
        "    pairbit1 = set()\n",
        "    pairbit2 = set()\n",
        "\n",
        "    for buck1 in cand_buck1:\n",
        "        if cand_buck1[buck1] > supp_thr:\n",
        "            pairbit1.add(buck1)\n",
        "\n",
        "    for buck2 in cand_buck2:\n",
        "        if cand_buck2[buck2] > supp_thr:\n",
        "            pairbit2.add(buck2)\n",
        "    \n",
        "    #Filter out the not-frequent singletons\n",
        "    freq_sets = filt(cand_sets, supp_thr)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Number of frequent singletons:\", len(freq_sets))\n",
        "        print(\"Number of frequent buckets 1:\", len(pairbit1), \"/\", buckets)\n",
        "        print(\"Number of frequent buckets 2:\", len(pairbit2), \"/\", buckets)\n",
        "        print(\"Time to count singletons:\", time.time() - t, \"sec\\n\")\n",
        "\n",
        "    #Deallocate the counts for singletons and buckets\n",
        "    del cand_sets\n",
        "    del cand_buck1\n",
        "    del cand_buck2\n",
        "    gc.collect()\n",
        "\n",
        "    #Save the freq. singletons in the list to return if necessary\n",
        "    if(min_card == 1):\n",
        "        all_freq_sets.extend(freq_sets)\n",
        "\n",
        "    #SECOND PASS                     \n",
        "    cand_pairs = dict()\n",
        "\n",
        "    t = time.time()\n",
        "\n",
        "    #Remove the not frequent singles from the basket\n",
        "    baskets[:] = [[item for item in b if (item,) in freq_sets] for b in baskets]\n",
        "\n",
        "    for b in baskets:\n",
        "        #Construct the candidate sets\n",
        "        for pair in it.combinations(b, 2):\n",
        "            index = p_t_i(pair)\n",
        "            buck1 = index % buckets\n",
        "            buck2 = p_t_i2(pair) % buckets\n",
        "            # Consider only the pairs in the frequent buckets\n",
        "            if (buck1 in pairbit1) and (buck2 in pairbit2):\n",
        "                if index not in cand_pairs:\n",
        "                    cand_pairs[index] = 1\n",
        "                else:\n",
        "                    cand_pairs[index] += 1\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Memory used to count pairs:\", sys.getsizeof(cand_pairs)/1000000,\"MB\")\n",
        "        print(\"Number of candidate pairs:\", len(cand_pairs))\n",
        "\n",
        "    #Deallocate the \"bitmaps\"\n",
        "    del pairbit1\n",
        "    del pairbit2\n",
        "    gc.collect()\n",
        "\n",
        "    #Filter out the not-frequent pairs\n",
        "    freq_sets = filt_pairs(cand_pairs, supp_thr, num_of_items)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Number of frequent pairs:\", len(freq_sets))\n",
        "        print(\"Time to count pairs:\", time.time() - t, \"sec\\n\")\n",
        "\n",
        "    #Deallocate the counts for pairs\n",
        "    del cand_pairs\n",
        "    gc.collect()\n",
        "    \n",
        "    #Save the freq. pairs in the list to return if necessary\n",
        "    if(min_card <= 2 and 2 <= max_card):\n",
        "        all_freq_sets.extend(freq_sets)\n",
        "\n",
        "    #K>=3 PASSES\n",
        "    k = 3\n",
        "    while len(freq_sets) != 0 and k <= max_card:\n",
        "        t = time.time()\n",
        "\n",
        "        cand_sets = construct(freq_sets, baskets, k, itemfilter=itemfilter)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Memory used to count {k}sets:\", sys.getsizeof(cand_sets)/1000000,\"MB\")\n",
        "            print(f\"Number of candidate {k}sets:\", len(cand_sets))\n",
        "\n",
        "        #Filter out the not-frequent k-sets\n",
        "        freq_sets = filt(cand_sets, supp_thr)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Number of frequent {k}sets:\", len(freq_sets))\n",
        "            print(f\"Time to count {k}sets:\", time.time() - t,\"sec\\n\")\n",
        "\n",
        "        #Deallocate the counts for k-sets\n",
        "        del cand_sets\n",
        "        gc.collect()\n",
        "\n",
        "        #print(f\"Elapsed time on {k} iteration: \", time.time() - t)\n",
        "\n",
        "        if(min_card <= k and len(freq_sets) != 0):\n",
        "            all_freq_sets.extend(freq_sets)\n",
        "        \n",
        "        k += 1\n",
        "\n",
        "    return all_freq_sets\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ka2kTcaB-J3"
      },
      "source": [
        "Here, follows the implementation of a time-limited version of Apriori algorithm useful for the parameter tuning of the support threshold (which is next)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-9cBd52FLNo"
      },
      "source": [
        "import signal\n",
        "import resource\n",
        "import copy\n",
        "from contextlib import contextmanager\n",
        "\n",
        "@contextmanager\n",
        "def timeout(time):\n",
        "    signal.signal(signal.SIGALRM, raise_timeout)\n",
        "    signal.alarm(time)\n",
        "\n",
        "    try:\n",
        "        yield\n",
        "    except TimeoutError:\n",
        "        pass\n",
        "    finally:\n",
        "        signal.signal(signal.SIGALRM, signal.SIG_IGN)\n",
        "\n",
        "def raise_timeout(signum, frame):\n",
        "    raise TimeoutError\n",
        "\n",
        "def apriori_timeout(baskets, supp_thr, min_card = 2, max_card = float('inf'), expire = 300):\n",
        "    with timeout(expire):\n",
        "        return apriori(baskets, supp_thr, min_card, max_card)\n",
        "\n",
        "    raise Exception(\"Timeout expired\")\n",
        "\n",
        "def support_tuning(baskets, t_limit, min_card = 2, max_card = float('inf'), precision = 0.01, verbose=False):\n",
        "    left = 1\n",
        "    right = len(baskets)\n",
        "\n",
        "    while (right - left) / right > precision:\n",
        "        m = (right + left) // 2\n",
        "\n",
        "        if verbose:\n",
        "            print(\"left =\", left, \"supp =\", m, \"right =\", right)\n",
        "\n",
        "        b = copy.deepcopy(baskets)\n",
        "        try:\n",
        "            t0 = time.time()\n",
        "            apriori_timeout(b, m, expire=t_limit)\n",
        "            t1 = time.time()\n",
        "            if verbose:\n",
        "                print(\"Elapsed\", t1-t0, \"seconds\")\n",
        "\n",
        "            right = m\n",
        "        except:\n",
        "            if verbose:\n",
        "                print(\"Timeout\")\n",
        "\n",
        "            left = m\n",
        "    \n",
        "    return right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY_EtJWZK_lA"
      },
      "source": [
        "Here we defined a function to study the time consumption of the algorithms we presented"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bruhdb2AH4u5"
      },
      "source": [
        "import copy\n",
        "import time\n",
        "\n",
        "def time_complexity(baskets, thresholds, buckets=0, itemfilter=1):\n",
        "    \"\"\"\n",
        "    time_complexity takes as parameters:\n",
        "    1. the original dataset in baskets\n",
        "    2. a list of support thresholds as thresholds\n",
        "    3. the number of buckets for PCY (if not specified or zero, runs Apriori)\n",
        "    4. the itemfiltering we want to use\n",
        "    and prints the elapsed time within every computation of the desired\n",
        "        algorithm for each support threshold\n",
        "    \"\"\"\n",
        "    for s in thresholds:\n",
        "        b = copy.deepcopy(baskets)\n",
        "        if buckets == 0:\n",
        "            t0 = time.time()\n",
        "            apriori(b, s, itemfilter=itemfilter)\n",
        "            t1 = time.time()\n",
        "\n",
        "            print(\"Time elapsed using support threshold =\", s, \"with Apriori using item filter #\", itemfilter,\")\", t1 - t0, \"seconds\")\n",
        "        else:\n",
        "            t0 = time.time()\n",
        "            pcy(b, s, baskets=buckets, itemfilter=itemfilter)\n",
        "            t1 = time.time()\n",
        "            \n",
        "            print(\"Time elapsed using support threshold =\", s, \"with Apriori using item filter #\", itemfilter,\" and buckets\", buckets,\")\", t1 - t0, \"seconds\")\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGpSmchUEiEO"
      },
      "source": [
        "Loads the dataset in the list of lists B"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtKYdaszFJvT"
      },
      "source": [
        "B = load_baskets('title.principals.tsv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd5OardqEoZk"
      },
      "source": [
        "Here, we use the verbose option on the 4 versions of the code we wanted to analyse."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAxO0kwSZGVl",
        "outputId": "44962435-af75-4482-fd01-2e4d091d0440"
      },
      "source": [
        "bask = copy.deepcopy(B)\n",
        "F = apriori(bask, 300, verbose=True, itemfilter=0)\n",
        "\n",
        "print(\"*****************************\")\n",
        "\n",
        "bask = copy.deepcopy(B)\n",
        "F = apriori(bask, 300, verbose=True, itemfilter=1)\n",
        "\n",
        "print(\"*****************************\")\n",
        "\n",
        "bask = copy.deepcopy(B)\n",
        "F = apriori(bask, 300, verbose=True, itemfilter=2)\n",
        "\n",
        "print(\"*****************************\")\n",
        "\n",
        "bask = copy.deepcopy(B)\n",
        "F = pcy(bask, 300, 80000, verbose=True, itemfilter=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory used to count singletons: 83.886184 MB\n",
            "Number of candidate singletons: 1868056\n",
            "Number of frequent singletons: 6014\n",
            "Time to count singletons: 10.211458921432495 sec\n",
            "\n",
            "Memory used to count pairs: 5.242984 MB\n",
            "Number of candidate pairs: 121151\n",
            "Number of frequent pairs: 3892\n",
            "Time to count pairs: 12.896682500839233 sec\n",
            "\n",
            "Memory used to count 3sets: 10.485872 MB\n",
            "Number of candidate 3sets: 262902\n",
            "Number of frequent 3sets: 2773\n",
            "Time to count 3sets: 3.5084221363067627 sec\n",
            "\n",
            "Memory used to count 4sets: 10.485872 MB\n",
            "Number of candidate 4sets: 319465\n",
            "Number of frequent 4sets: 1774\n",
            "Time to count 4sets: 3.279869794845581 sec\n",
            "\n",
            "Memory used to count 5sets: 10.485872 MB\n",
            "Number of candidate 5sets: 285349\n",
            "Number of frequent 5sets: 1066\n",
            "Time to count 5sets: 2.718259334564209 sec\n",
            "\n",
            "Memory used to count 6sets: 10.485872 MB\n",
            "Number of candidate 6sets: 198122\n",
            "Number of frequent 6sets: 541\n",
            "Time to count 6sets: 2.047327995300293 sec\n",
            "\n",
            "Memory used to count 7sets: 5.242984 MB\n",
            "Number of candidate 7sets: 103075\n",
            "Number of frequent 7sets: 192\n",
            "Time to count 7sets: 1.5230634212493896 sec\n",
            "\n",
            "Memory used to count 8sets: 1.310824 MB\n",
            "Number of candidate 8sets: 36704\n",
            "Number of frequent 8sets: 41\n",
            "Time to count 8sets: 1.2040560245513916 sec\n",
            "\n",
            "Memory used to count 9sets: 0.295016 MB\n",
            "Number of candidate 9sets: 7773\n",
            "Number of frequent 9sets: 4\n",
            "Time to count 9sets: 1.1048305034637451 sec\n",
            "\n",
            "Memory used to count 10sets: 0.036976 MB\n",
            "Number of candidate 10sets: 731\n",
            "Number of frequent 10sets: 0\n",
            "Time to count 10sets: 1.0531392097473145 sec\n",
            "\n",
            "*****************************\n",
            "Memory used to count singletons: 83.886184 MB\n",
            "Number of candidate singletons: 1868056\n",
            "Number of frequent singletons: 6014\n",
            "Time to count singletons: 9.725402116775513 sec\n",
            "\n",
            "Memory used to count pairs: 5.242984 MB\n",
            "Number of candidate pairs: 121151\n",
            "Number of frequent pairs: 3892\n",
            "Time to count pairs: 12.296984910964966 sec\n",
            "\n",
            "Memory used to count 3sets: 0.589936 MB\n",
            "Number of candidate 3sets: 16622\n",
            "Number of frequent 3sets: 2773\n",
            "Time to count 3sets: 4.099376201629639 sec\n",
            "\n",
            "Memory used to count 4sets: 0.295016 MB\n",
            "Number of candidate 4sets: 5766\n",
            "Number of frequent 4sets: 1774\n",
            "Time to count 4sets: 2.2984461784362793 sec\n",
            "\n",
            "Memory used to count 5sets: 0.073832 MB\n",
            "Number of candidate 5sets: 2142\n",
            "Number of frequent 5sets: 1066\n",
            "Time to count 5sets: 1.7739474773406982 sec\n",
            "\n",
            "Memory used to count 6sets: 0.036976 MB\n",
            "Number of candidate 6sets: 1342\n",
            "Number of frequent 6sets: 541\n",
            "Time to count 6sets: 1.478489637374878 sec\n",
            "\n",
            "Memory used to count 7sets: 0.018536 MB\n",
            "Number of candidate 7sets: 558\n",
            "Number of frequent 7sets: 192\n",
            "Time to count 7sets: 1.2693862915039062 sec\n",
            "\n",
            "Memory used to count 8sets: 0.004712 MB\n",
            "Number of candidate 8sets: 153\n",
            "Number of frequent 8sets: 41\n",
            "Time to count 8sets: 1.1759328842163086 sec\n",
            "\n",
            "Memory used to count 9sets: 0.000248 MB\n",
            "Number of candidate 9sets: 4\n",
            "Number of frequent 9sets: 4\n",
            "Time to count 9sets: 1.1234486103057861 sec\n",
            "\n",
            "Memory used to count 10sets: 0.000248 MB\n",
            "Number of candidate 10sets: 0\n",
            "Number of frequent 10sets: 0\n",
            "Time to count 10sets: 1.1038620471954346 sec\n",
            "\n",
            "*****************************\n",
            "Memory used to count singletons: 83.886184 MB\n",
            "Number of candidate singletons: 1868056\n",
            "Number of frequent singletons: 6014\n",
            "Time to count singletons: 9.72117018699646 sec\n",
            "\n",
            "Memory used to count pairs: 5.242984 MB\n",
            "Number of candidate pairs: 121151\n",
            "Number of frequent pairs: 3892\n",
            "Time to count pairs: 12.241227865219116 sec\n",
            "\n",
            "Memory used to count 3sets: 0.295016 MB\n",
            "Number of candidate 3sets: 6126\n",
            "Number of frequent 3sets: 2773\n",
            "Time to count 3sets: 31.592283248901367 sec\n",
            "\n",
            "Memory used to count 4sets: 0.147568 MB\n",
            "Number of candidate 4sets: 3515\n",
            "Number of frequent 4sets: 1774\n",
            "Time to count 4sets: 4.154923439025879 sec\n",
            "\n",
            "Memory used to count 5sets: 0.073832 MB\n",
            "Number of candidate 5sets: 1903\n",
            "Number of frequent 5sets: 1066\n",
            "Time to count 5sets: 2.1539227962493896 sec\n",
            "\n",
            "Memory used to count 6sets: 0.036976 MB\n",
            "Number of candidate 6sets: 1229\n",
            "Number of frequent 6sets: 541\n",
            "Time to count 6sets: 0.92563796043396 sec\n",
            "\n",
            "Memory used to count 7sets: 0.018536 MB\n",
            "Number of candidate 7sets: 557\n",
            "Number of frequent 7sets: 192\n",
            "Time to count 7sets: 0.4277985095977783 sec\n",
            "\n",
            "Memory used to count 8sets: 0.004712 MB\n",
            "Number of candidate 8sets: 153\n",
            "Number of frequent 8sets: 41\n",
            "Time to count 8sets: 0.18016672134399414 sec\n",
            "\n",
            "Memory used to count 9sets: 0.000248 MB\n",
            "Number of candidate 9sets: 4\n",
            "Number of frequent 9sets: 4\n",
            "Time to count 9sets: 0.13613629341125488 sec\n",
            "\n",
            "Memory used to count 10sets: 0.000248 MB\n",
            "Number of candidate 10sets: 0\n",
            "Number of frequent 10sets: 0\n",
            "Time to count 10sets: 0.027611255645751953 sec\n",
            "\n",
            "*****************************\n",
            "Memory used to count singletons: 83.886184 MB\n",
            "Memory used to count buckets 1: 2.621552 MB\n",
            "Memory used to count buckets 2: 2.621552 MB\n",
            "Number of candidate singletons: 1868056\n",
            "Number of frequent singletons: 6014\n",
            "Number of frequent buckets 1: 55925 / 80000\n",
            "Number of frequent buckets 2: 56046 / 80000\n",
            "Time to count singletons: 89.44966411590576 sec\n",
            "\n",
            "Memory used to count pairs: 2.621552 MB\n",
            "Number of candidate pairs: 74730\n",
            "Number of frequent pairs: 3892\n",
            "Time to count pairs: 19.492507934570312 sec\n",
            "\n",
            "Memory used to count 3sets: 0.295016 MB\n",
            "Number of candidate 3sets: 6126\n",
            "Number of frequent 3sets: 2773\n",
            "Time to count 3sets: 31.601965188980103 sec\n",
            "\n",
            "Memory used to count 4sets: 0.147568 MB\n",
            "Number of candidate 4sets: 3515\n",
            "Number of frequent 4sets: 1774\n",
            "Time to count 4sets: 4.192021131515503 sec\n",
            "\n",
            "Memory used to count 5sets: 0.073832 MB\n",
            "Number of candidate 5sets: 1903\n",
            "Number of frequent 5sets: 1066\n",
            "Time to count 5sets: 2.184849500656128 sec\n",
            "\n",
            "Memory used to count 6sets: 0.036976 MB\n",
            "Number of candidate 6sets: 1229\n",
            "Number of frequent 6sets: 541\n",
            "Time to count 6sets: 0.9648761749267578 sec\n",
            "\n",
            "Memory used to count 7sets: 0.018536 MB\n",
            "Number of candidate 7sets: 557\n",
            "Number of frequent 7sets: 192\n",
            "Time to count 7sets: 0.443129301071167 sec\n",
            "\n",
            "Memory used to count 8sets: 0.004712 MB\n",
            "Number of candidate 8sets: 153\n",
            "Number of frequent 8sets: 41\n",
            "Time to count 8sets: 0.18440890312194824 sec\n",
            "\n",
            "Memory used to count 9sets: 0.000248 MB\n",
            "Number of candidate 9sets: 4\n",
            "Number of frequent 9sets: 4\n",
            "Time to count 9sets: 0.13564395904541016 sec\n",
            "\n",
            "Memory used to count 10sets: 0.000248 MB\n",
            "Number of candidate 10sets: 0\n",
            "Number of frequent 10sets: 0\n",
            "Time to count 10sets: 0.021082401275634766 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3haeLihF8NV"
      },
      "source": [
        "Here, we compute the time consumption relative to different values of support threshold using the Apriori algorithm without any item filter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6SbkgaeauPv",
        "outputId": "16b82e23-69b3-4865-e793-d2f83f1cddb4"
      },
      "source": [
        "time_complexity(B, [4000, 3000, 2000, 1500, 1000, 700, 500, 300])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time elapsed using support threshold = 4000 with Apriori using item filter # 1 ) 19.45033836364746 seconds\n",
            "Time elapsed using support threshold = 3000 with Apriori using item filter # 1 ) 25.00787878036499 seconds\n",
            "Time elapsed using support threshold = 2000 with Apriori using item filter # 1 ) 27.495540380477905 seconds\n",
            "Time elapsed using support threshold = 1500 with Apriori using item filter # 1 ) 30.509689569473267 seconds\n",
            "Time elapsed using support threshold = 1000 with Apriori using item filter # 1 ) 31.70386552810669 seconds\n",
            "Time elapsed using support threshold = 700 with Apriori using item filter # 1 ) 35.450013160705566 seconds\n",
            "Time elapsed using support threshold = 500 with Apriori using item filter # 1 ) 44.35437846183777 seconds\n",
            "Time elapsed using support threshold = 300 with Apriori using item filter # 1 ) 49.795989751815796 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Js4hDyMrPfVn",
        "outputId": "aa757ae3-0f2a-4e3a-8228-f2e22f387cb5"
      },
      "source": [
        "print(\"Support threshold obtained with 20 seconds:\", support_tuning(B, 20))\n",
        "print(\"Support threshold obtained with 25 seconds:\", support_tuning(B, 25))\n",
        "print(\"Support threshold obtained with 30 seconds:\", support_tuning(B, 30))\n",
        "print(\"Support threshold obtained with 35 seconds:\", support_tuning(B, 35))\n",
        "print(\"Support threshold obtained with 40 seconds:\", support_tuning(B, 40))\n",
        "print(\"Support threshold obtained with 45 seconds:\", support_tuning(B, 45))\n",
        "print(\"Support threshold obtained with 50 seconds:\", support_tuning(B, 50))\n",
        "print(\"Support threshold obtained with 55 seconds:\", support_tuning(B, 55))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Support threshold obtained with 20 seconds: 931512\n",
            "Support threshold obtained with 25 seconds: 2369\n",
            "Support threshold obtained with 30 seconds: 1749\n",
            "Support threshold obtained with 35 seconds: 804\n",
            "Support threshold obtained with 40 seconds: 564\n",
            "Support threshold obtained with 45 seconds: 504\n",
            "Support threshold obtained with 50 seconds: 490\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9RNjHAC23m-"
      },
      "source": [
        "This is a basic implementation of the SON algorithm having as input parameters:\n",
        "1. the list of the baskets\n",
        "2. the fraction p of baskets to have in each chunk\n",
        "3. the support threshold\n",
        "4. the minimal and the maximal cardinality of the frequent itemsets to be returned (optional)\n",
        "\n",
        "The main phases are the following:\n",
        "1. it splits the baskets in chunks according to the passed fraction p\n",
        "2. runs the apriori algorithm (alternatively we can use the PCY) on each chunk\n",
        "with support threshold p*supp_thr and gets the candidate frequent itemsets\n",
        "3. filters out the false positive itemsets by scanning all the baskets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnRWYhAi3mtS"
      },
      "source": [
        "import math\n",
        "import copy\n",
        "\n",
        "def son(baskets, p, supp_thr, min_card = 2, max_card = float('inf')):\n",
        "    \n",
        "    #Calculates the chunk length and the number of chunks\n",
        "    chunk_len = int(math.ceil(len(baskets)*p))\n",
        "    num_chunks = int(math.ceil(len(baskets)/chunk_len))\n",
        "    \n",
        "    #Set of the candidate frequent itemsets\n",
        "    cand_itemsets = set()\n",
        "    \n",
        "    #Runs the passed algorithm on all the chunks and gets\n",
        "    #the set of candidate frequent itemsets\n",
        "    for i in range(num_chunks):\n",
        "        cand_itemsets.update(apriori(copy.deepcopy(baskets[chunk_len*i:chunk_len*(i+1)]), supp_thr*p, min_card, max_card))    \n",
        "\n",
        "    #Dictionary to count the occurrences of each candidate \n",
        "    #frequent itemset on the set of baskets\n",
        "    count = dict([(e,0)for e in cand_itemsets])\n",
        "\n",
        "    #Counts their occurrencies    \n",
        "    for b in baskets:\n",
        "        for kset in cand_itemsets:\n",
        "            if set(kset).issubset(set(b)):\n",
        "                count[kset] += 1\n",
        "\n",
        "    #Return only the itemsets with count >= than the support threshold\n",
        "    return [kset for kset, count in count.items() if count >= supp_thr]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYjyXrTuGs3a"
      },
      "source": [
        "Here, we set up Spark and Hadooop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzsZMczr80Y_"
      },
      "source": [
        "#Setting up spark and hadoop\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www-eu.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.1.2-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init(\"spark-3.1.2-bin-hadoop2.7\")# SPARK_HOME\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "import pyspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVacg6gG7g_m"
      },
      "source": [
        "This is the implementation of the SON algorithm by using \n",
        "the Map-Reduce framework with Spark and Hadoop.\n",
        "\n",
        "The main phases are the following:\n",
        "1. First map function: takes the chunk and returns the candidate frequent itemsets by using the A-Priori algorithm with support threshold p*supp_thr. The output is a set of key value pairs (kset, 1) where kset is a frequent itemset from the chunk.\n",
        "2. First reduce function: simply ignores the value and produce the key of the itemset. The output is the set of candidate frequent itemsets.\n",
        "3. Second map function: takes the set of candidate frequent itemsets and a chunk and returns the number of occurrences of each of the candidate itemsets among the baskets in the chunk. The output is a set of key value pairs (kset, count) where kset is the candidate frequent itemset and count is the number of occurrencies in the chunk.\n",
        "4. Second reduce function: returns the sum of the occurrencies of a given itemset.\n",
        "5. Those itemsets whose count on all the baskets is >= than the support threshold are returned as frequent itemsets (filtering of false positives)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJIO3lNy89ld"
      },
      "source": [
        "import math\n",
        "\n",
        "def count_sets(chunk, sets):\n",
        "    \"\"\"\n",
        "        Function that counts the number of occurrencies\n",
        "        of the itemsets in the baskets contained in the chunk.\n",
        "        \n",
        "        chunk: list of baskets\n",
        "        sets: list of candidate frequent itemsets\n",
        "\n",
        "        return: a list with pairs (kset, count) indicating\n",
        "        the number of occurrencies of each itemset in the chunk\n",
        "    \"\"\"\n",
        "    counts = dict([(kset, 0)for kset, value in sets])\n",
        "       \n",
        "    for b in chunk:\n",
        "        for kset, value in sets:\n",
        "            if set(kset).issubset(set(b)):\n",
        "                counts[kset] += 1\n",
        "\n",
        "    return [(kset, count) for kset, count in counts.items()]\n",
        "\n",
        "def map_reduce_son(baskets, p, supp_thr, min_card = 2, max_card = float('inf'), algo = apriori):\n",
        "\n",
        "    sc = spark.sparkContext\n",
        "\n",
        "    chunk_len = int(math.ceil(len(baskets)*p))\n",
        "    num_chunks = int(math.ceil(len(baskets)/chunk_len))\n",
        "    \n",
        "    rdd = sc.parallelize([baskets[chunk_len*i:chunk_len*(i+1)] for i in range(num_chunks)])\n",
        "    \n",
        "    #First map-reduce step\n",
        "    candidate_sets = (rdd.flatMap(lambda chunk: algo(chunk, supp_thr*p, min_card, max_card))\n",
        "               .map(lambda kset: (kset, 1))\n",
        "               .reduceByKey(lambda a, b: a)\n",
        "               .collect())\n",
        "    \n",
        "    #Second map-reduce step\n",
        "    out = (rdd.flatMap(lambda chunk: count_sets(chunk, candidate_sets))\n",
        "               .reduceByKey(lambda a, b: a+b)\n",
        "               .collect())\n",
        "    \n",
        "    #Filter out the false positives and return the freq. itemsets\n",
        "    return [kset for kset, count in out if count >= 3000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2gGudl3Jml_"
      },
      "source": [
        "Here we check the correctness of the algorithm by comparing the results with the standard APriori."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQAHBkqp_efK"
      },
      "source": [
        "Baskets = copy.deepcopy(B)\n",
        "\n",
        "print(map_reduce_son(B, 0.3, 3000))\n",
        "print(apriori(Baskets, 3000))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}